---
title: 2.5 梯度下降和最小二乘法
categories:      
    Deep Learning    
tags: [梯度下降和最小二乘法,Algorithm]
date: 2019-1-26 12:18:03
---

# 摘要

本节笔记主要记录梯度下降算法和最小二乘法的相同点和不同点，为了简单的理解和推导，我们暂只讨论一元线性回归下的梯度下降算法和最小二乘。从实现++代码上的最大不同就是梯度下降采用了迭代算法---预先给一个参数设置初始值，然后通过迭代和学习率实现线性方程直线的无限靠近理想函数。最小二乘采用了高中数学求凸函数（下凹）的最小值---即求导数为零的参数方程。

- [x] Edit By Porter, 积水成渊,蛟龙生焉。 

<!-- more -->

梯度下降和最小二乘发的相同点和不同点如下：

## 狭义的最小二乘法：

指的是在线性回归下采用最小二乘准则（或者说叫做最小平方），进行线性拟合参数求解的、矩阵形式的公式方法，是线性假设下的一种有闭式解的参数求解方法，最终结果为全局最优；

## 广义的最小二乘法：

是最小二乘准则，本质上是一种evaluation rule或者说objective funcion。是一种对于偏差程度的评估准则。

* 梯度下降法，是假设条件更为广泛（无约束）的，一种通过迭代更新来逐步进行的参数优化方法，最终结果为局部最优；

## 最小二乘与极大似然的区别

* 二乘法：即观测值与实际数据误差平方和最小，其没有假设，几何意义上就是距离最小

* 最大似然估计：估计参数可能值，使样本发生概率最大

## 对于回归问题，两者对于最优参数的不同解法

假设一元回归方程为

$$
f(x_i)=\omega x_i +b
$$

实际中我们会采样得到关于[x,y]的很多样本，此时我们会通过损失函数loss，来求最优$$\omega , b$$的最优解。

$$
loss = (\omega x + b -y)^2
$$

假设$$E_(\omega, b)=loss_(\omega, b)$$,于是：

为求得最优的$$\omega ， b$$,即求解$$E_(\omega, b)=\sum_{m}^{i=1} (y_i - \omega x_i -b)^2$$的最小取值下的$$\omega ， b$$最优解。

* 注意此时的$$E_(\omega, b)$$是一个凸函数，注意也就是对应国内教材下凹函数曲线。

## 最小二乘法的求解最优的$$\omega ， b$$的方法

直接通过对$$E_(\omega, b)$$对$$\omega ， b$$分别求偏导，并将偏导设为0，求取凸函数取得最小值下的对应$$\omega ， b$$参数。

### 最小二乘法的局限性和适用场景　　

　　　　从上面可以看出，最小二乘法适用简洁高效，比梯度下降这样的迭代法似乎方便很多。但是这里我们就聊聊最小二乘法的局限性。

　　　　首先，最小二乘法需要计算的逆矩阵，有可能它的逆矩阵不存在，这样就没有办法直接用最小二乘法了，此时梯度下降法仍然可以使用。当然，我们可以通过对样本数据进行整理，去掉冗余特征。让的行列式不为0，然后继续使用最小二乘法。

　　　　第二，当样本特征n非常的大的时候，计算的逆矩阵是一个非常耗时的工作（nxn的矩阵求逆），甚至不可行。此时以梯度下降为代表的迭代法仍然可以使用。那这个n到底多大就不适合最小二乘法呢？如果你没有很多的分布式大数据计算资源，建议超过10000个特征就用迭代法吧。或者通过主成分分析降低特征的维度后再用最小二乘法。

　　　　第三，如果拟合函数不是线性的，这时无法使用最小二乘法，需要通过一些技巧转化为线性才能使用，此时梯度下降仍然可以用。

　　　　第四，讲一些特殊情况。当样本量m很少，小于特征数n的时候，这时拟合方程是欠定的，常用的优化方法都无法去拟合数据。当样本量m等于特征说n的时候，用方程组求解就可以了。当m大于n时，拟合方程是超定的，也就是我们常用与最小二乘法的场景了。

 原文地址：http://www.cnblogs.com/pinard/p/5976811.html

### 梯度下降算法的求解最优的$$\omega ， b$$的方法

同样，梯度下降算法也是要求最小损失函数

$$
loss = (\omega x + b -y)^2
$$

但是方法不一样了，梯度下降算法，在上面的基础上设定一个学习率的参数，假设为:$$lr$$

对于待求解的参数$$\omega ， b$$则转化为 --> 求最终待求的$$\omega ' ， b'$$关于前一时刻的$$\omega ， b$$的函数，如下

$$\omega '=\omega - lr * \frac{\bigtriangledown loss}{\bigtriangledown \omega}$$

同理：

$$b '=b - lr * \frac{\bigtriangledown loss}{\bigtriangledown b}$$

* 根据上面的式子，假设学习率lr的初始值为0，然后通过for循环进行迭代，此时求解的关于$$\omega ， b$$的曲线会随着迭代的次数增加，一步步无限的靠近最优的解，理论上会求解到最优的$$\omega ， b$$。


## 总结

以上就是关于简单的一元回归的关于梯度下降和最小二乘的区别，对于代码中，的迭代的实现下一小节中会有详细的介绍，会通过代码来直观例说迭代算法和递归算法的差异和原理。

无聊时刻看部综艺：

<iframe height=498 width=100% src='http://player.youku.com/embed/XNDAwMjkxOTM4OA==' frameborder=0 'allowfullscreen'></iframe>