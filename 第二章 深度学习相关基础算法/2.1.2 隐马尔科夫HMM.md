# 2.1.2 隐马尔科夫HMM

在讲HMM前先看看什么是熵的概念

## 一、什么是熵(Entropy)

信息熵的概念这个得从 **热熵** 开始说起，信熵是香农老先生从热力学引进来的，为了表示把信息中排除了冗余后的平均信息量称为“信息熵”。

- 热力学中的热熵是表示分子状态混乱程度的物理量。香农用信息熵的概念来描述信源的不确定度。



### 1.1 热力学中--热熵

从能量角度来看,熵定律意味着自然进行的能量转化过程总是由有序度高的能量向有序度低的能量转化,这个过程必定朝着熵增加的方向进行。

高温物体所有分子的平均能量要高于低温物体,所以相接触时总是从高温物体向低温物体传递热量,因为碰撞使它们的状态向平衡过渡,系统才会稳定;

热熵是向着熵增大的方向进行。从宇宙形成到地球诞生以及地球生命的形成,热熵一直有缓慢变大的趋势。

![熵增](./image2/hotShang_1.1.jpeg)

#### 热熵改变是指在某个空间内热量分布的变化。

- 1、在热力学中熵是对热量状态的描述，空间内热量分布差异越大则熵越小，做功的能力越强；空间内热量分布差异越小则熵越大，做功的能力越弱。热熵改变是指在某个空间内热量分布的变化，熵越大则热量分布的差异越小。正常情况下熵会从小到大的变化，最终熵达到最大而呈热寂。

- 2、根据热力学第二定律，作为一个“孤立”的系统，宇宙的熵会随着时间的流逝而增加，由有序向无序，当宇宙的熵达到最大值时，宇宙中的其他有效能量已经全数转化为热能，所有物质温度达到热平衡。这种状态称为热寂。这样的宇宙中再也没有任何可以维持运动或是生命的能量存在。
- 3、热熵改变的本质是热量分布由有序向无序发展，所以也可以看成系统混乱程度的改变。
- 4、熵最初是根据热力学第二定律引出的一个反映自发过程不可逆性的物质状态参量。热力学第二定律是根据大量观察结果总结出来的规律，有下述表述方式：热量总是从高温物体传到低温物体，不可能作相反的传递而不引起其他的变化；功可以全部转化为热，但任何热机不能全部地，连续不断地把所接受的热量转变为功；在孤立系统中，实际发生过程总使整个系统的熵值增大，此即熵增原理。 
> 简单总结：热熵是熵增的过程，热量由有序的状态，转移，或是机械能，或是向其他低温物体转移，产生的影响是，最终达到系统平衡的目的，但也使得整个系统不再是以前清晰的状态，相反是变成了另一种混乱的状态，可以这样白话理解。

#### 信息量

信息量：一个事件发生的概率越小，信息量越大，所以信息量应该为概率的减函数，对于相互独立的两个事有p(xy)=p(x)p(y)，对于这两个事件信息量应满足h(xy)=h(x)+h(y)，那么信息量应为对数函数：

![信息量](./image2/HMM_xinxiL.png)

### 1.2 信息论中的信息熵--信熵

- 香农用信息熵的概念来描述信源的不确定度。

根据Charles H. Bennett对Maxwell's Demon的重新解释，对信息的销毁是一个不可逆过程，所以销毁信息是符合热力学第二定律的。而产生信息，则是为系统引入负（热力学）熵的过程。所以信息熵的符号与热力学熵应该是相反的。

* 假设离散随机变量X的概率分布为P(x)，则其熵为：

![熵增](./image2/hotShang_1.2.jpeg)

H(x) = E[I(xi)] = E[ log(2,1/p(xi)) ] = -∑p(xi)log(2,p(xi)) (i=1,2,..n)

其中，x表示随机变量，与之相对应的是所有可能输出的集合，定义为符号集,随机变量的输出用x表示。P(x)表示输出概率函数。**变量的不确定性越大，熵也就越大** ，把它搞清楚所需要的信息量也就越大.

能量角度，高温向低温转变，一般是熵增的过程；而信息论中，为了最大可能接收到正确的（发出==收到）的信息，我们处理（优化）系统，是一种熵减的过程，信熵越小，系统有用信息量越大。

信息熵：**信息的基本作用就是消除人们对事物的不确定性** 。多数粒子组合之后，在它似像非像的形态上押上有价值的数码，具体地说，这就是一个在博弈对局中现象信息的混乱。

#### 举个例子

-(p1*log(2,p1) + p2 * log(2,p2) +　．．．　+p32 *log(2,p32))，其中，p1，p2 ，　．．．，p32 分别是这 32 个球队夺冠的概率。香农把它称为“信息熵” (Entropy)，一般用符号 H 表示，单位是比特。

![熵增](./image2/hotShang_1.3.jpeg)

有兴趣的读者可以推算一下当 32 个球队夺冠概率相同时，对应的信息熵等于五比特。有数学基础的读者还可以证明上面公式的值不可能大于五。因为得冠军的频率相同代表整个系统信息量最小。


熵是随机变量不确定性的度量，不确定性越大，熵值就越大；若随机变量退化成定值，熵为0。均匀分布(信熵最大)是“最不确定”的分布。

熵最早来原于物理学. 德国物理学家鲁道夫·克劳修斯首次提出熵的概念，用来表示任何一种能量在空间中分布的均匀程度，能量分布得越均匀，熵就越大。

> 总结：香农，描述一个信息系统的时候就借用了熵的概念，这里熵表示的是这个信息系统的平均信息量(平均不确定程度),信熵越小，系统信息不确定程度越低，反之，系统信息输出越混乱，有用信息越不容易被确认。

### 1.3 联合熵

联合熵是一集变量之间不确定性的衡量手段。两个变量和的联合信息熵定义为：

![联合熵](./image2/lianheShang1.3.1.png)

- 一集变量的联合熵大于或等于这集变量中任一个的独立熵。

![联合熵](./image2/lianheShang1.3.2.png) 

- 少于独立熵的和

![联合熵](./image2/lianheShang1.3.3.png) 

这表明，两个变量关联之后不确定性会增大，但是又由于相互有制约关系，不确定小于单独两个变量的不确定度之和。

### 1.4 条件熵

- 条件熵H(X|Y) = H(X,Y) - H(Y)

> X在条件Y下的条件熵

![条件熵](./image2/tiaojianShang1.4.1.png) 

条件熵 H(Y|X) 表示在已知随机变量 X 的条件下随机变量 Y 的不确定性。条件熵 H(Y|X) 定义为 X 给定条件下 Y 的条件概率分布的熵对  X 的数学期望：

证明如下：

![条件熵](./image2/HMM_tiaojianS_1.4.1.png)

条件熵 H(Y|X) 相当于联合熵 H(X,Y) 减去单独的熵 H(X)，即

![条件熵](./image2/HMM_tiaojianS_1.4.2.png)

举个例子，比如环境温度是低还是高，和我穿短袖还是外套这两个事件可以组成联合概率分布 H(X,Y)，因为两个事件加起来的信息量肯定是大于单一事件的信息量的。假设 H(X) 对应着今天环境温度的信息量，由于今天环境温度和今天我穿什么衣服这两个事件并不是独立分布的，所以在已知今天环境温度的情况下，我穿什么衣服的信息量或者说不确定性是被减少了(条件熵减少，联合熵是不变)。当已知 H(X) 这个信息量的时候，H(X,Y) 剩下的信息量就是条件熵：

H(Y|X)=H(X,Y)−H(X)






### 1.5 相对熵与互信息

- 相对熵 (Relative entropy)，也称KL散度 (Kullback–Leibler divergence)

 设p(x),q(x)是X中取值的两个概率分布，则p对q的相对熵是：



![相对熵](./image2/HMM_xiangduiS_1.5.1.gif)

性质： 

- 1、如果 p(x) 和 q(x) 两个分布相同，那么相对熵等于0
- 2、![相对熵](./image2/HMM_xiangduiS_1.5.2.gif) ,相对熵具有不对称性。大家可以举个简单例子算一下。
- 3、DKL(p||q)≥0 证明如下（利用Jensen不等式https://en.wikipedia.org/wiki/Jensen%27s_inequality）：

![相对熵](./image2/HMM_xiangduiS_1.5.3.png)

因为：

![相对熵](./image2/HMM_xiangduiS_1.5.4.gif)

所以：

![相对熵](./image2/HMM_xiangduiS_1.5.5.gif)

总结：相对熵可以用来衡量两个概率分布之间的差异，上面公式的意义就是求 p 与 q 之间的对数差在 p 上的期望值。

> 相对熵，两者相同，那么相对熵等于0，两者差距越大，则相对熵值越大。

### 1.6 交叉熵 (Cross entropy)

熵的公式：
$$
H(p)=-\displaystyle\sum_{x}p(x)logp(x)
$$

相对熵的公式：

$$
H(p,q)=\displaystyle\sum _{x}p(x)log\frac{1}{q(x)}=-\sum _{x}p(x)logq(x)
$$

所以有：

$$
D_{KL}(p||q)=H(p,q)-H(p)
$$

> 当用非真实分布 q(x) 得到的平均码长比真实分布 p(x) 得到的平均码长多出的比特数就是相对熵）

> 并且当 H(p) 为常量时（注：在机器学习中，训练数据分布是固定的），最小化相对熵 DKL(p||q) 等价于最小化交叉熵 H(p,q) 也等价于最大化似然估计（具体参考Deep Learning 5.5）。

> 在机器学习中，我们希望在训练数据上模型学到的分布 P(model) 和真实数据的分布  P(real) 越接近越好，所以我们可以使其相对熵最小。

- 信息熵是衡量随机变量分布的混乱程度，是随机分布各事件发生的信息量的期望值，随机变量的取值个数越多，状态数也就越多，信息熵就越大，混乱程度就越大。当随机分布为均匀分布时，熵最大；信息熵推广到多维领域，则可得到联合信息熵；条件熵表示的是在 X 给定条件下，Y 的条件概率分布的熵对 X
的期望。
- 相对熵可以用来衡量两个概率分布之间的差异。
- 交叉熵可以来衡量在给定的真实分布下，使用非真实分布所指定的策略消除系统的不确定性所需要付出的努力的大小。


## 二、[最大熵模型](https://www.cnblogs.com/hgl0417/p/6693656.html)

* 最大熵模型在形式上是最漂亮的统计模型，而在实现上是最复杂的模型之一。最大熵模型，可以说是集简与繁于一体，形式简单，实现复杂。值得一提的是，在Google的很多产品中，比如机器翻译，都直接或间接地用到了最大熵模型。 

* 达拉皮垂兄弟等科学家在那里，用于最大熵模型和其他一些先进的数学工具对股票预测，获得了巨大的成功。(值得一提的是，信息处理的很多数学手段，包括隐含马尔可夫模型、子波变换、贝叶斯网络等等，在华尔街多有直接的应用。由此可见，数学模型的作用)。

我们平常说的最大熵模型，只是运用最大熵思想的多分类模型，最大熵的思想却是一种通用的思维方法。所以，理解最大熵模型只需要搞清楚两件事就可以：

- 最大熵思想是什么
- 最大熵模型是如何运用最大熵思想的

> 分类模型有判别模型和生成模型两种，判别模型是要学习一个条件概率分布 P(y|x)。



## 三、HMM（隐马尔可夫模型）